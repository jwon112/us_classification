#!/usr/bin/env python3
"""
í†µí•© ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse

def load_results(results_dir):
    """ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
    csv_path = os.path.join(results_dir, "integrated_experiment_results.csv")
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Results file not found: {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š Loaded {len(df)} results from {csv_path}")
    return df

def create_performance_comparison(df, save_dir):
    """ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    
    # Baseline vs Enhanced ë¹„êµ
    plt.figure(figsize=(15, 10))
    
    # 1. Accuracy ë¹„êµ
    plt.subplot(2, 2, 1)
    baseline_acc = df[df['model_type'] == 'baseline'].groupby('model_name')['final_accuracy'].mean()
    enhanced_acc = df[df['model_type'] == 'enhanced'].groupby('model_name')['final_accuracy'].mean()
    
    x = np.arange(len(baseline_acc))
    width = 0.35
    
    plt.bar(x - width/2, baseline_acc.values, width, label='Baseline', alpha=0.8, color='skyblue')
    plt.bar(x + width/2, enhanced_acc.values, width, label='Enhanced', alpha=0.8, color='orange')
    
    plt.xlabel('Model')
    plt.ylabel('Accuracy')
    plt.title('Baseline vs Enhanced Models - Accuracy Comparison')
    plt.xticks(x, [name.upper() for name in baseline_acc.index], rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. F1 Score ë¹„êµ
    plt.subplot(2, 2, 2)
    baseline_f1 = df[df['model_type'] == 'baseline'].groupby('model_name')['final_f1'].mean()
    enhanced_f1 = df[df['model_type'] == 'enhanced'].groupby('model_name')['final_f1'].mean()
    
    plt.bar(x - width/2, baseline_f1.values, width, label='Baseline', alpha=0.8, color='skyblue')
    plt.bar(x + width/2, enhanced_f1.values, width, label='Enhanced', alpha=0.8, color='orange')
    
    plt.xlabel('Model')
    plt.ylabel('F1 Score')
    plt.title('Baseline vs Enhanced Models - F1 Score Comparison')
    plt.xticks(x, [name.upper() for name in baseline_f1.index], rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. Parameter Count ë¹„êµ
    plt.subplot(2, 2, 3)
    baseline_params = df[df['model_type'] == 'baseline'].groupby('model_name')['total_params'].first()
    enhanced_params = df[df['model_type'] == 'enhanced'].groupby('model_name')['total_params'].first()
    
    plt.bar(x - width/2, baseline_params.values, width, label='Baseline', alpha=0.8, color='skyblue')
    plt.bar(x + width/2, enhanced_params.values, width, label='Enhanced', alpha=0.8, color='orange')
    
    plt.xlabel('Model')
    plt.ylabel('Parameter Count')
    plt.title('Baseline vs Enhanced Models - Parameter Count Comparison')
    plt.xticks(x, [name.upper() for name in baseline_params.index], rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    # 4. Accuracy vs Parameters (Scatter plot)
    plt.subplot(2, 2, 4)
    for model_type in ['baseline', 'enhanced']:
        subset = df[df['model_type'] == model_type]
        plt.scatter(subset['total_params'], subset['final_accuracy'], 
                   label=model_type.capitalize(), alpha=0.7, s=50)
    
    plt.xlabel('Parameter Count')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Parameter Count')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xscale('log')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š Performance comparison chart saved")

def create_seed_analysis(df, save_dir):
    """ì‹œë“œë³„ ë¶„ì„ ì°¨íŠ¸"""
    
    plt.figure(figsize=(15, 10))
    
    # 1. ì‹œë“œë³„ í‰ê·  ì„±ëŠ¥
    plt.subplot(2, 2, 1)
    seed_performance = df.groupby(['seed', 'model_type'])['final_accuracy'].mean().unstack()
    seed_performance.plot(kind='bar', ax=plt.gca())
    plt.title('Average Performance by Seed and Model Type')
    plt.xlabel('Seed')
    plt.ylabel('Average Accuracy')
    plt.legend(title='Model Type')
    plt.xticks(rotation=0)
    plt.grid(True, alpha=0.3)
    
    # 2. ì‹œë“œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„í¬
    plt.subplot(2, 2, 2)
    for model_type in ['baseline', 'enhanced']:
        subset = df[df['model_type'] == model_type]
        plt.hist(subset['final_accuracy'], alpha=0.7, label=model_type.capitalize(), bins=15)
    
    plt.xlabel('Accuracy')
    plt.ylabel('Frequency')
    plt.title('Accuracy Distribution by Model Type')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. ì‹œë“œë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    plt.subplot(2, 2, 3)
    best_per_seed = df.loc[df.groupby('seed')['final_accuracy'].idxmax()]
    best_per_seed.plot(x='seed', y='final_accuracy', kind='bar', ax=plt.gca())
    plt.title('Best Model Performance by Seed')
    plt.xlabel('Seed')
    plt.ylabel('Best Accuracy')
    plt.xticks(rotation=0)
    plt.grid(True, alpha=0.3)
    
    # 4. ì‹œë“œë³„ ì„±ëŠ¥ ë³€ë™ì„±
    plt.subplot(2, 2, 4)
    seed_variability = df.groupby('seed')['final_accuracy'].std()
    plt.bar(seed_variability.index, seed_variability.values, alpha=0.8, color='lightcoral')
    plt.title('Performance Variability by Seed (Standard Deviation)')
    plt.xlabel('Seed')
    plt.ylabel('Standard Deviation')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'seed_analysis.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š Seed analysis chart saved")

def create_model_ranking(df, save_dir):
    """ëª¨ë¸ ìˆœìœ„ ì°¨íŠ¸"""
    
    # ì „ì²´ í‰ê·  ì„±ëŠ¥ìœ¼ë¡œ ìˆœìœ„ ê²°ì •
    model_ranking = df.groupby(['model_type', 'model_name'])['final_accuracy'].mean().sort_values(ascending=False)
    
    plt.figure(figsize=(12, 8))
    
    # ìƒ‰ìƒ ë§¤í•‘
    colors = ['skyblue' if 'baseline' in idx else 'orange' for idx in model_ranking.index]
    
    bars = plt.bar(range(len(model_ranking)), model_ranking.values, color=colors, alpha=0.8)
    
    # ë¼ë²¨ ì„¤ì •
    labels = [f"{model_type.upper()}_{name.upper()}" for (model_type, name) in model_ranking.index]
    plt.xticks(range(len(model_ranking)), labels, rotation=45, ha='right')
    
    # ê°’ í‘œì‹œ
    for i, (bar, value) in enumerate(zip(bars, model_ranking.values)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{value:.4f}', ha='center', va='bottom', fontsize=9)
    
    plt.title('Model Performance Ranking (All Models)')
    plt.xlabel('Model')
    plt.ylabel('Average Accuracy')
    plt.grid(True, alpha=0.3)
    
    # ë²”ë¡€
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='skyblue', alpha=0.8, label='Baseline'),
                      Patch(facecolor='orange', alpha=0.8, label='Enhanced')]
    plt.legend(handles=legend_elements)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'model_ranking.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š Model ranking chart saved")
    
    # ìˆœìœ„ ì¶œë ¥
    print("\nğŸ† MODEL PERFORMANCE RANKING:")
    print("="*50)
    for i, ((model_type, model_name), accuracy) in enumerate(model_ranking.items(), 1):
        print(f"{i:2d}. {model_type.upper():10} {model_name.upper():12} | Accuracy: {accuracy:.4f}")

def create_detailed_statistics(df, save_dir):
    """ìƒì„¸ í†µê³„ ìƒì„±"""
    
    # ìš”ì•½ í†µê³„
    summary_stats = df.groupby(['model_type', 'model_name']).agg({
        'final_accuracy': ['count', 'mean', 'std', 'min', 'max'],
        'final_f1': ['mean', 'std'],
        'final_precision': ['mean', 'std'],
        'final_recall': ['mean', 'std'],
        'total_params': 'first',
        'best_accuracy': ['mean', 'std']
    }).round(4)
    
    # CSVë¡œ ì €ì¥
    summary_path = os.path.join(save_dir, "detailed_statistics.csv")
    summary_stats.to_csv(summary_path)
    
    # LaTeX í…Œì´ë¸”ë¡œë„ ì €ì¥
    latex_path = os.path.join(save_dir, "detailed_statistics.tex")
    with open(latex_path, 'w') as f:
        f.write(summary_stats.to_latex())
    
    print(f"ğŸ“ˆ Detailed statistics saved to CSV and LaTeX")
    
    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š DETAILED STATISTICS:")
    print("="*80)
    print(summary_stats)
    
    return summary_stats

def main():
    parser = argparse.ArgumentParser(description='Analyze Integrated Experiment Results')
    parser.add_argument('--results_dir', type=str, required=True, 
                       help='Path to results directory')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory for analysis (default: results_dir)')
    
    args = parser.parse_args()
    
    if args.output_dir is None:
        args.output_dir = args.results_dir
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("ğŸ” Starting Integrated Results Analysis")
    print(f"ğŸ“ Results directory: {args.results_dir}")
    print(f"ğŸ“‚ Output directory: {args.output_dir}")
    
    try:
        # ê²°ê³¼ ë¡œë“œ
        df = load_results(args.results_dir)
        
        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“Š Dataset Overview:")
        print(f"   Total experiments: {len(df)}")
        print(f"   Seeds: {sorted(df['seed'].unique())}")
        print(f"   Model types: {df['model_type'].unique()}")
        print(f"   Models: {sorted(df['model_name'].unique())}")
        
        # ì°¨íŠ¸ ìƒì„±
        create_performance_comparison(df, args.output_dir)
        create_seed_analysis(df, args.output_dir)
        create_model_ranking(df, args.output_dir)
        
        # ìƒì„¸ í†µê³„
        summary_stats = create_detailed_statistics(df, args.output_dir)
        
        print(f"\nğŸ‰ Analysis completed successfully!")
        print(f"ğŸ“‚ Results saved in: {args.output_dir}")
        
    except Exception as e:
        print(f"\nâŒ Analysis failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
